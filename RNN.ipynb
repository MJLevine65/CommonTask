{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ab23eb3-6e04-40c4-8272-09a22b4985d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch,sklearn\n",
    "from sklearn import model_selection\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torcheval import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "dca37ab2-eb09-498f-875d-d57609ab8fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(datastr : str,features,target):\n",
    "    def month_fun(m):\n",
    "        d = {1:0,2:0,3:0,4:0,5:0,6:0,7:0,8:0,9:0,10:0,11:0,12:0}\n",
    "        d[m] += 1.0\n",
    "        return d\n",
    "    def stat_fun(s):\n",
    "        d = {'USW00012839' : 0, 'USW00014819' :0, 'USW00013904' :0, 'USW00094728': 0}\n",
    "        d[s] += 1.0\n",
    "        return d\n",
    "\n",
    "    dataset = pd.read_csv(datastr)\n",
    "    dataset = dataset[features+[target,\"Month\",\"STATION\"]]\n",
    "    dataset.dropna(inplace=True)\n",
    "    dataset[target] = dataset[target].astype(\"float\")\n",
    "    for f in features:\n",
    "        dataset[f] = dataset[f].astype(\"float\")\n",
    "    dataset[[\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"]] = \\\n",
    "        dataset.apply(lambda x : month_fun(x[\"Month\"]),axis = 1,result_type=\"expand\")\n",
    "    dataset[['USW00012839', 'USW00014819', 'USW00013904', 'USW00094728']] =\\\n",
    "            dataset.apply(lambda x : stat_fun(x[\"STATION\"]),axis = 1, result_type=\"expand\")\n",
    "    dataset.dropna(inplace=True)\n",
    "    dataset.drop(columns = [\"Month\",\"STATION\"],inplace = True)\n",
    "    train,test = model_selection.train_test_split(dataset,test_size = 0.1,random_state = 3)\n",
    "    train_x = train.drop(columns = [target])\n",
    "    train_y = train[target]\n",
    "    test_x = test.drop(columns = [target])\n",
    "    test_y = test[target]\n",
    "    return train_x,train_y,test_x,test_y\n",
    "\n",
    "class NN(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size):\n",
    "        super(NN,self).__init__()\n",
    "        # define layers\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size = self.input_size, hidden_size = self.hidden_size,\\\n",
    "        dtype = torch.float32,batch_first=True,nonlinearity = 'relu',dropout = 0.1)\n",
    "        self.rnn2 = nn.RNN(input_size = self.input_size, hidden_size = self.hidden_size,\\\n",
    "        dtype = torch.float32,batch_first=True)\n",
    "        self.h_n = torch.zeros(1,64,self.hidden_size).requires_grad_()\n",
    "    # define forward function\n",
    "    def forward(self, rnninput,inputs):\n",
    "        rnninput = rnninput.view(rnninput.shape[0],rnninput.shape[1],1).float()\n",
    "        inputs = inputs.view(inputs.shape[0],inputs.shape[1]).float()\n",
    "        x, self.h_ = self.rnn(rnninput,self.h_n.detach())\n",
    "        x2, _ = self.rnn2(rnninput,self.h_n.detach())\n",
    "        x = nn.ReLU()(x)\n",
    "        # print(x[:,-1,:].size())\n",
    "        # print(inputs.size())\n",
    "        x = torch.cat((inputs,x[:,-1,:]),1)\n",
    "        # print(x[0])\n",
    "        # print(x[:,-1,:][0])\n",
    "        # print(x[:,-1,:][0])\n",
    "        x = nn.Linear(23,1,dtype=torch.float32)(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        # print(x[:5])\n",
    "        # x = nn.ReLU()(x)\n",
    "        # x = nn.Linear(16,8,dtype=torch.float32)(x)\n",
    "        # x = nn.ReLU()(x)\n",
    "        # x = nn.Linear(8,1,dtype=torch.float32)(x)\n",
    "        return x\n",
    "\n",
    "model = NN(1,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "1a856917-d37d-4d3e-b788-435defbfbfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in range(1,8):\n",
    "    data.append(\"d\" + str(i))\n",
    "train_x,train_y,test_x,test_y = load_data(\"1948_2024.csv\",data,\"TMAX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "833c5e76-5cef-4c35-a25e-b54993744d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[49. 58. 62. ...  0.  1.  0.]\n",
      " [57. 64. 46. ...  0.  1.  0.]\n",
      " [90. 91. 91. ...  0.  1.  0.]\n",
      " ...\n",
      " [87. 88. 87. ...  0.  0.  1.]\n",
      " [90. 87. 88. ...  0.  0.  1.]\n",
      " [57. 61. 76. ...  0.  0.  1.]]\n",
      "Epoch no. 1 0 % |total_loss:  4613967.016494751 | epoch_duration:  14.7 sec\n",
      "tensor([6.5231, 4.0229, 3.5935, 6.1075, 5.3612, 5.2792, 6.7285, 6.2363, 6.1386,\n",
      "        6.6137], grad_fn=<SliceBackward0>)\n",
      "Epoch no. 2 0 % |total_loss:  4277077.119384766 | epoch_duration:  9.75 sec\n",
      "tensor([126.8399,  79.0681,  69.8445, 119.9180,  96.5752, 100.2279, 126.2088,\n",
      "        121.4228, 117.6229, 124.4627], grad_fn=<SliceBackward0>)\n",
      "Epoch no. 3 0 % |total_loss:  4291664.039161682 | epoch_duration:  21.07 sec\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "Epoch no. 4 0 % |total_loss:  4260525.412567139 | epoch_duration:  10.35 sec\n",
      "tensor([79.7684, 50.6585, 44.8584, 76.2479, 59.6183, 63.0219, 81.6568, 75.9692,\n",
      "        75.4932, 78.3317], grad_fn=<SliceBackward0>)\n",
      "Epoch no. 5 0 % |total_loss:  4479860.77822876 | epoch_duration:  9.61 sec\n",
      "tensor([117.4586,  75.2984,  65.1219, 113.5842,  86.4431,  92.4886, 121.6889,\n",
      "        111.0755, 112.4149, 115.7190], grad_fn=<SliceBackward0>)\n",
      "Epoch no. 6 0 % |total_loss:  4199432.241119385 | epoch_duration:  19.33 sec\n",
      "tensor([17.6292, 11.3412,  9.7003, 17.0327, 12.9616, 13.9257, 18.3052, 16.5606,\n",
      "        17.0750, 17.4418], grad_fn=<SliceBackward0>)\n",
      "Epoch no. 7 0 % |total_loss:  4334356.770584106 | epoch_duration:  21.58 sec\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "Epoch no. 8 0 % |total_loss:  4432061.846313477 | epoch_duration:  9.65 sec\n",
      "tensor([49.5094, 32.6242, 27.1984, 48.2705, 36.0837, 39.1515, 52.7687, 46.0538,\n",
      "        48.9192, 48.5496], grad_fn=<SliceBackward0>)\n",
      "Epoch no. 9 0 % |total_loss:  4464127.727325439 | epoch_duration:  9.42 sec\n",
      "tensor([88.0279, 58.2556, 48.8326, 86.3484, 64.6825, 69.9945, 94.6753, 81.8121,\n",
      "        87.2040, 86.8038], grad_fn=<SliceBackward0>)\n",
      "Epoch no. 10 0 % |total_loss:  4334017.876998901 | epoch_duration:  9.3 sec\n",
      "tensor([0.8651, 0.4333, 0.4785, 1.0361, 0.9759, 0.8336, 1.0424, 0.7853, 0.9339,\n",
      "        1.1551], grad_fn=<SliceBackward0>)\n",
      "Epoch no. 11 0 % |total_loss:  4552462.759399414 | epoch_duration:  20.35 sec\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "Epoch no. 12 0 % |total_loss:  4520323.4524383545 | epoch_duration:  15.77 sec\n",
      "tensor([33.9783, 23.1480, 17.5374, 33.1975, 24.8614, 27.1195, 37.1174, 30.4291,\n",
      "        34.6708, 33.5975], grad_fn=<SliceBackward0>)\n",
      "Epoch no. 13 0 % |total_loss:  4357849.717178345 | epoch_duration:  9.39 sec\n",
      "tensor([102.2648,  72.2230,  53.5966, 100.4363,  75.8304,  82.6197, 113.9290,\n",
      "         90.7366, 105.5531, 101.5135], grad_fn=<SliceBackward0>)\n",
      "Epoch no. 14 0 % |total_loss:  4329658.100738525 | epoch_duration:  9.47 sec\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "Epoch no. 15 0 % |total_loss:  4390088.344604492 | epoch_duration:  9.3 sec\n",
      "tensor([ 90.2427,  63.8112,  47.8358,  88.9176,  67.0236,  73.5558, 100.9030,\n",
      "         80.0812,  93.6483,  89.0733], grad_fn=<SliceBackward0>)\n",
      "Epoch no. 16 0 % |total_loss:  4318006.610809326 | epoch_duration:  26.38 sec\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "Epoch no. 17 0 % |total_loss:  4381534.487426758 | epoch_duration:  9.32 sec\n",
      "tensor([102.7048,  72.1955,  54.7372, 100.9113,  76.5138,  83.2060, 114.3925,\n",
      "         91.8393, 105.9564, 101.6379], grad_fn=<SliceBackward0>)\n",
      "Epoch no. 18 0 % |total_loss:  4370734.565864563 | epoch_duration:  9.21 sec\n",
      "tensor([108.6105,  75.8564,  58.0910, 106.6229,  80.6014,  87.6300, 120.1365,\n",
      "         97.5052, 111.4941, 107.3692], grad_fn=<SliceBackward0>)\n",
      "Epoch no. 19 0 % |total_loss:  4334032.1873931885 | epoch_duration:  9.13 sec\n",
      "tensor([36.8217, 27.2508, 18.9985, 35.8294, 27.3506, 30.4101, 41.4354, 31.5987,\n",
      "        38.4662, 36.3509], grad_fn=<SliceBackward0>)\n",
      "Epoch no. 20 0 % |total_loss:  4373184.898895264 | epoch_duration:  9.55 sec\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "Epoch no. 21 0 % |total_loss:  4307457.80078125 | epoch_duration:  9.83 sec\n",
      "tensor([103.9825,  76.3406,  53.9208, 101.7247,  78.4376,  86.3701, 117.7967,\n",
      "         89.5145, 108.9890, 103.2891], grad_fn=<SliceBackward0>)\n",
      "Epoch no. 22 0 % |total_loss:  4393799.767791748 | epoch_duration:  22.71 sec\n",
      "tensor([85.3400, 61.9241, 44.7460, 83.1896, 64.3311, 70.5326, 96.3325, 74.1596,\n",
      "        89.3869, 84.5096], grad_fn=<SliceBackward0>)\n",
      "Epoch no. 23 0 % |total_loss:  4241349.691421509 | epoch_duration:  9.5 sec\n",
      "tensor([17.3250, 12.4913,  9.3523, 16.8360, 12.8417, 14.1590, 19.5795, 15.2193,\n",
      "        17.9760, 17.0850], grad_fn=<SliceBackward0>)\n",
      "Epoch no. 24 0 % |total_loss:  4367490.332046509 | epoch_duration:  11.85 sec\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "Epoch no. 25 0 % |total_loss:  4350589.69342041 | epoch_duration:  9.38 sec\n",
      "tensor([37.3168, 27.5906, 19.6129, 36.3379, 28.1902, 30.7605, 41.7932, 32.3940,\n",
      "        38.9398, 37.0534], grad_fn=<SliceBackward0>)\n",
      "Epoch no. 26 0 % |total_loss:  4357741.762145996 | epoch_duration:  10.33 sec\n",
      "tensor([58.2660, 43.2544, 30.7308, 57.2990, 44.5921, 49.1632, 66.2937, 49.9304,\n",
      "        61.3020, 58.0257], grad_fn=<SliceBackward0>)\n",
      "Epoch no. 27 0 % |total_loss:  4439615.346847534 | epoch_duration:  9.37 sec\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "Epoch no. 28 0 % |total_loss:  4294415.435058594 | epoch_duration:  9.52 sec\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "Epoch no. 29 0 % |total_loss:  4416082.453613281 | epoch_duration:  9.32 sec\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "Epoch no. 30 0 % |total_loss:  4397284.744293213 | epoch_duration:  22.62 sec\n",
      "tensor([109.5897,  82.0857,  57.9689, 106.7785,  83.3966,  91.8905, 123.4986,\n",
      "         93.7025, 114.7243, 108.6841], grad_fn=<SliceBackward0>)\n",
      "Epoch no. 31 0 % |total_loss:  4528042.999954224 | epoch_duration:  22.11 sec\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "Epoch no. 32 0 % |total_loss:  4379768.865692139 | epoch_duration:  9.19 sec\n",
      "tensor([110.2771,  83.5003,  58.4361, 107.1408,  84.4355,  92.8943, 124.6762,\n",
      "         93.7594, 115.6773, 109.4735], grad_fn=<SliceBackward0>)\n",
      "Epoch no. 33 0 % |total_loss:  4346222.9945373535 | epoch_duration:  23.07 sec\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "Epoch no. 34 0 % |total_loss:  4471430.504684448 | epoch_duration:  24.91 sec\n",
      "tensor([86.2854, 65.5256, 46.5822, 84.2850, 66.5704, 73.2531, 97.4174, 73.5797,\n",
      "        90.5625, 85.8902], grad_fn=<SliceBackward0>)\n",
      "Epoch no. 35 0 % |total_loss:  4307148.526290894 | epoch_duration:  27.67 sec\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "Epoch no. 36 0 % |total_loss:  4343098.559013367 | epoch_duration:  6.96 sec\n",
      "tensor([103.2769,  77.4087,  56.6565, 100.5537,  79.2864,  86.3853, 115.6204,\n",
      "         89.8990, 107.8312, 102.5250], grad_fn=<SliceBackward0>)\n",
      "Epoch no. 37 0 % |total_loss:  4499753.6075286865 | epoch_duration:  6.99 sec\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "Epoch no. 38 0 % |total_loss:  4330043.160949707 | epoch_duration:  7.0 sec\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "Epoch no. 39 0 % |total_loss:  4509743.1775894165 | epoch_duration:  7.1 sec\n",
      "tensor([71.3333, 54.2488, 39.2266, 69.2356, 55.2286, 60.4470, 80.4637, 61.1888,\n",
      "        74.8061, 70.8545], grad_fn=<SliceBackward0>)\n",
      "Epoch no. 40 0 % |total_loss:  4433838.206787109 | epoch_duration:  8.91 sec\n",
      "tensor([129.0575,  95.8921,  72.6739, 127.2795,  99.1303, 107.8511, 143.8932,\n",
      "        114.4252, 134.0812, 127.9334], grad_fn=<SliceBackward0>)\n",
      "Epoch no. 41 0 % |total_loss:  4376978.474655151 | epoch_duration:  7.04 sec\n",
      "tensor([87.0292, 65.1188, 48.3380, 84.7249, 67.0698, 73.0798, 97.1280, 76.0594,\n",
      "        90.8042, 86.1525], grad_fn=<SliceBackward0>)\n",
      "Epoch no. 42 0 % |total_loss:  4314075.843399048 | epoch_duration:  7.06 sec\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "Epoch no. 43 0 % |total_loss:  4492034.283721924 | epoch_duration:  29.99 sec\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "Epoch no. 44 0 % |total_loss:  4417211.604171753 | epoch_duration:  16.8 sec\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "Epoch no. 45 0 % |total_loss:  4502953.614181519 | epoch_duration:  10.69 sec\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "Epoch no. 46 0 % |total_loss:  4348559.173049927 | epoch_duration:  9.28 sec\n",
      "tensor([59.3016, 45.2371, 33.2931, 58.4308, 46.2512, 50.6570, 66.5513, 51.5074,\n",
      "        62.0227, 58.9375], grad_fn=<SliceBackward0>)\n",
      "Epoch no. 47 0 % |total_loss:  4243904.097137451 | epoch_duration:  9.31 sec\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "Epoch no. 48 0 % |total_loss:  4366707.05519104 | epoch_duration:  9.25 sec\n",
      "tensor([57.8409, 44.6788, 32.7671, 56.0580, 45.1708, 49.7125, 64.9437, 49.5669,\n",
      "        60.6062, 57.2285], grad_fn=<SliceBackward0>)\n",
      "Epoch no. 49 0 % |total_loss:  4274906.368560791 | epoch_duration:  15.82 sec\n",
      "tensor([101.0385,  79.0303,  56.9374,  99.0742,  79.5031,  87.2524, 114.1912,\n",
      "         86.1971, 106.1390, 100.7089], grad_fn=<SliceBackward0>)\n",
      "Epoch no. 50 0 % |total_loss:  4140405.9276275635 | epoch_duration:  24.19 sec\n",
      "tensor([15.3430, 11.7462,  8.9183, 14.8129, 11.9241, 13.2336, 17.3365, 13.3617,\n",
      "        16.1815, 14.9770], grad_fn=<SliceBackward0>)\n",
      "Epoch no. 51 0 % |total_loss:  4480289.217056274 | epoch_duration:  9.84 sec\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "Epoch no. 52 0 % |total_loss:  4270085.756622314 | epoch_duration:  19.49 sec\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "Epoch no. 53 0 % |total_loss:  4416701.564407349 | epoch_duration:  32.48 sec\n",
      "tensor([66.3167, 51.5310, 38.3152, 64.8878, 52.1553, 57.0457, 74.3925, 57.4045,\n",
      "        69.3504, 65.8024], grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[393], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs,labels)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Calculate gradient of matrix with requires_grad = True\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#computes dloss/dx for every parameter x which has requires_grad=True\u001b[39;00m\n\u001b[0;32m     36\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep() \u001b[38;5;66;03m# x += -lr * x.grad ie updates the weights of the parameters\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Adding loss to total loss\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "tl0 = np.inf\n",
    "\n",
    "# Defining optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.03)\n",
    "print(train_x.values)\n",
    "train_loader = DataLoader(list(zip(torch.tensor(train_x.values),torch.tensor(train_y.values))),batch_size  = 64)\n",
    "test_loader = DataLoader(list(zip(torch.tensor(test_x.values),torch.tensor(test_y.values))),batch_size  = 64)\n",
    "import time\n",
    "epoch = 0\n",
    "while True:\n",
    "\n",
    "    # Initialising statistics that we will be tracking across epochs\n",
    "    start_time = time.time()\n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        if len(inputs) < 64:\n",
    "            continue\n",
    "        optimizer.zero_grad() #Set all graidents to zero for each step as they accumulate over backprop\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        inputs = inputs.view(inputs.shape[0], -1)\n",
    "        _,inputs,inputs2 = torch.tensor_split(inputs,((0,7)),-1)\n",
    "        outputs = model.forward(inputs,inputs2).view(inputs.shape[0])\n",
    "        #print(outputs[:5])\n",
    "        labels = labels.float()\n",
    "        loss = criterion(outputs,labels)\n",
    "\n",
    "        # Calculate gradient of matrix with requires_grad = True\n",
    "        loss.backward() #computes dloss/dx for every parameter x which has requires_grad=True\n",
    "        optimizer.step() # x += -lr * x.grad ie updates the weights of the parameters\n",
    "        # Adding loss to total loss\n",
    "        total_loss += loss.item()\n",
    "    end_time = time.time() - start_time\n",
    "\n",
    "    # Printing out statistics\n",
    "    print(\"Epoch no.\",epoch+1 , round(0, 3),\"%\", \"|total_loss: \", total_loss, \"| epoch_duration: \", round(end_time,2),\"sec\")\n",
    "    epoch += 1\n",
    "    print(outputs[:10])\n",
    "    if abs(total_loss - tl0) < 1:\n",
    "        break\n",
    "    else:\n",
    "        tl0 = total_loss\n",
    "inputs = torch.tensor(test_x.values)\n",
    "labels = torch.tensor(test_y.values)\n",
    "# inputs = inputs.view(inputs.shape[0], -1)\n",
    "outputs = model.forward(inputs,None)\n",
    "outputs = outputs[0].view(inputs.shape[0])\n",
    "metricR2 = metrics.R2Score()\n",
    "metricR2.update(outputs,labels)\n",
    "print(\"R2\",metricR2.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35eed5fc-cb3a-4733-9815-1e1285dda983",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
