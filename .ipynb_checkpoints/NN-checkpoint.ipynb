{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch,sklearn\n",
    "from sklearn import model_selection\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torcheval import metrics\n",
    "import time\n",
    "import uuid\n",
    "import kalshi_python\n",
    "from kalshi_python.models import *\n",
    "import pickle\n",
    "\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self,input_size,h1,h2):\n",
    "        super().__init__()\n",
    "        # define layers\n",
    "        self.input_size = input_size\n",
    "        self.h1 = h1\n",
    "        self.h2 = h2\n",
    "        self.layers = self.get_fc_layers()\n",
    "\n",
    "    def get_fc_layers(self):\n",
    "        layers = nn.Sequential(\n",
    "            nn.Linear(self.input_size, self.h1,dtype = torch.float64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.h1, self.h2,dtype = torch.float64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.h2, 6,dtype = torch.float64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(6, 1,dtype = torch.float64),\n",
    "           )\n",
    "        return layers\n",
    "    # define forward function\n",
    "    def forward(self, input):\n",
    "        x = self.layers(input)\n",
    "        return x\n",
    "\n",
    "    def month_fun(self,m):\n",
    "            d = {1:0,2:0,3:0,4:0,5:0,6:0,7:0,8:0,9:0,10:0,11:0,12:0}\n",
    "            d[m] += 1.0\n",
    "            return d\n",
    "\n",
    "    def stat_fun(self,s):\n",
    "            d = {'USW00012839' : 0, 'USW00014819' :0, 'USW00013904' :0, 'USW00094728': 0}\n",
    "            d[s] += 1.0\n",
    "            return d\n",
    "\n",
    "    def load_data(self,datastr : str,features,target):\n",
    "    \n",
    "        dataset = pd.read_csv(datastr)\n",
    "        dataset = dataset[features+[target,\"STATION\",\"Month\"]]\n",
    "        dataset.dropna(inplace=True)\n",
    "        dataset[target] = dataset[target].astype(\"float\")\n",
    "        for f in features:\n",
    "            dataset[f] = dataset[f].astype(\"float\")\n",
    "        dataset[[\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"]] = \\\n",
    "            dataset.apply(lambda x : self.month_fun(x[\"Month\"]),axis = 1,result_type=\"expand\")\n",
    "        dataset[['USW00012839', 'USW00014819', 'USW00013904', 'USW00094728']] =\\\n",
    "                dataset.apply(lambda x : self.stat_fun(x[\"STATION\"]),axis = 1, result_type=\"expand\")\n",
    "        dataset.dropna(inplace=True)\n",
    "        dataset.drop(columns = [\"Month\",\"STATION\"],inplace = True)\n",
    "        train,test = model_selection.train_test_split(dataset,test_size = 0.1,random_state = 3)\n",
    "        train_x = train.drop(columns = [target])\n",
    "        train_y = train[target]\n",
    "        test_x = test.drop(columns = [target])\n",
    "        test_y = test[target]\n",
    "        self.train_x,self.train_y,self.test_x,self.test_y = train_x,train_y,test_x,test_y\n",
    "    def train(self):\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(self.parameters(), lr=0.003)\n",
    "        train_loader = DataLoader(list(zip(torch.tensor(self.train_x.values),torch.tensor(self.train_y.values))),batch_size  = 64)\n",
    "        \n",
    "        metric = metrics.R2Score()\n",
    "        epoch = 0\n",
    "        loss_prev = np.inf\n",
    "        while True:\n",
    "            epoch += 1\n",
    "        \n",
    "            # Initialising statistics that we will be tracking across epochs\n",
    "            start_time = time.time()\n",
    "            total_correct = 0\n",
    "            total_loss = 0\n",
    "        \n",
    "            for i, data in enumerate(train_loader, 0):\n",
    "                # get the inputs; data is a list of [inputs, labels]\n",
    "                inputs, labels = data\n",
    "                # zero the parameter gradients: Clean the gradient caclulated in the previous iteration\n",
    "                optimizer.zero_grad() #Set all graidents to zero for each step as they accumulate over backprop\n",
    "        \n",
    "                # forward + backward + optimize\n",
    "                inputs = inputs.view(inputs.shape[0], -1)\n",
    "                \n",
    "                outputs = self.forward(inputs).view(inputs.shape[0])\n",
    "                loss = criterion(outputs,labels)\n",
    "        \n",
    "                # Calculate gradient of matrix with requires_grad = True\n",
    "                loss.backward() #computes dloss/dx for every parameter x which has requires_grad=True\n",
    "                optimizer.step() # x += -lr * x.grad ie updates the weights of the parameters\n",
    "        \n",
    "                # Adding loss to total loss\n",
    "                total_loss += loss.item()\n",
    "            end_time = time.time() - start_time\n",
    "            total_loss /= len(self.train_x)\n",
    "            print(\"Epoch no.\",epoch ,\"|total_loss: \", total_loss, \"| epoch_duration: \", round(end_time,2),\"sec\")\n",
    "            # Printing out statistics\n",
    "            if abs(total_loss-loss_prev) < 0.0001:\n",
    "                break\n",
    "            else:\n",
    "                loss_prev = total_loss\n",
    "            \n",
    "        \n",
    "        inputs = torch.tensor(self.test_x.values)\n",
    "        labels = torch.tensor(self.test_y.values)\n",
    "        outputs = model.forward(inputs)\n",
    "        outputs = outputs.view(inputs.shape[0])\n",
    "        metricR2 = metrics.R2Score()\n",
    "        metricR2.update(outputs,labels)\n",
    "        metricR2.compute()\n",
    "        print(\"R2\",metricR2.compute())\n",
    "\n",
    "    def predict(self,inputs):\n",
    "        inputs = inputs[features+[target,\"STATION\",\"Month\"]]\n",
    "        inputs.dropna(inplace=True)\n",
    "        inputs[target] = inputs[target].astype(\"float\")\n",
    "        for f in features:\n",
    "            inputs[f] = inputs[f].astype(\"float\")\n",
    "        inputs[[\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"]] = \\\n",
    "            dataset.apply(lambda x : self.month_fun(x[\"Month\"]),axis = 1,result_type=\"expand\")\n",
    "        inputs[['USW00012839', 'USW00014819', 'USW00013904', 'USW00094728']] =\\\n",
    "                inputs.apply(lambda x : stat_fun(x[\"STATION\"]),axis = 1, result_type=\"expand\")\n",
    "        inputs.dropna(inplace=True)\n",
    "        inputs.drop(columns = [\"Month\",\"STATION\"],inplace = True)\n",
    "        return model.forward(inputs).view(inputs.shape[0])\n",
    "\n",
    "def make_orders(self,predictions,date):\n",
    "    config = kalshi_python.Configuration()\n",
    "    config.host = 'https://demo-api.kalshi.co/trade-api/v2'\n",
    "    client = kalshi_python.ApiInstance(\n",
    "    email=\"mlevine6@bu.edu\",\n",
    "    password= \"in5u>e@t}MaE:mM\",\n",
    "    configuration=config,\n",
    ")\n",
    "    tickers = [\"HIGHNY\",\"HIGHCHI\",\"HIGHMIA\",\"HIGHAUS\"]\n",
    "    tickers = [ticker + \"-\" + date for ticker in tickers]\n",
    "    for ticker in tickers:\n",
    "        pred = predictions[ticker]\n",
    "        response = kalshi_api.get_event(tickers[0])\n",
    "        temps = [float(o.ticker.split(\"-\")[-1][1:]) for o in response.markets]\n",
    "        diffs = [abs(pred-t) for t in temps]\n",
    "        pick = [o.ticker for o in response.markets][np.argmin(diffs)]\n",
    "        if exchangeStatus.trading_active:\n",
    "            orderUuid = str(uuid.uuid4())\n",
    "            orderResponse = kalshi_api.create_order(CreateOrderRequest(\n",
    "                ticker=pick,\n",
    "                action='buy',\n",
    "                type='limit',\n",
    "                yes_price=50,\n",
    "                count=1,\n",
    "                client_order_id=orderUuid,\n",
    "                side='yes',\n",
    "            ))\n",
    "            print(orderresponse)\n",
    "            \n",
    "            \n",
    "        \n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch no. 1 |total_loss:  1.8121912037580779 | epoch_duration:  0.8 sec\n",
      "Epoch no. 2 |total_loss:  0.5195702076735446 | epoch_duration:  0.79 sec\n",
      "Epoch no. 3 |total_loss:  0.4921518197052369 | epoch_duration:  0.75 sec\n",
      "Epoch no. 4 |total_loss:  0.4866019884995567 | epoch_duration:  0.78 sec\n",
      "Epoch no. 5 |total_loss:  0.4825170120010136 | epoch_duration:  0.75 sec\n",
      "Epoch no. 6 |total_loss:  0.4790328329913065 | epoch_duration:  0.82 sec\n",
      "Epoch no. 7 |total_loss:  0.4710233281964587 | epoch_duration:  0.77 sec\n",
      "Epoch no. 8 |total_loss:  0.4551418400930584 | epoch_duration:  0.75 sec\n",
      "Epoch no. 9 |total_loss:  0.437743013809906 | epoch_duration:  0.76 sec\n",
      "Epoch no. 10 |total_loss:  0.4131635042417949 | epoch_duration:  0.76 sec\n",
      "Epoch no. 11 |total_loss:  0.3873030022759302 | epoch_duration:  0.83 sec\n",
      "Epoch no. 12 |total_loss:  0.3608156859319652 | epoch_duration:  0.82 sec\n",
      "Epoch no. 13 |total_loss:  0.3325535435857284 | epoch_duration:  0.79 sec\n",
      "Epoch no. 14 |total_loss:  0.3078587706232149 | epoch_duration:  0.73 sec\n",
      "Epoch no. 15 |total_loss:  0.2896180263589339 | epoch_duration:  0.78 sec\n",
      "Epoch no. 16 |total_loss:  0.27822780533287045 | epoch_duration:  0.81 sec\n",
      "Epoch no. 17 |total_loss:  0.26954586615770004 | epoch_duration:  0.77 sec\n",
      "Epoch no. 18 |total_loss:  0.2642198667380218 | epoch_duration:  0.76 sec\n",
      "Epoch no. 19 |total_loss:  0.2586880460223179 | epoch_duration:  0.76 sec\n",
      "Epoch no. 20 |total_loss:  0.25468648442887226 | epoch_duration:  0.75 sec\n",
      "Epoch no. 21 |total_loss:  0.2510351166011863 | epoch_duration:  0.79 sec\n",
      "Epoch no. 22 |total_loss:  0.24927302318255692 | epoch_duration:  0.79 sec\n",
      "Epoch no. 23 |total_loss:  0.2473646176961523 | epoch_duration:  0.76 sec\n",
      "Epoch no. 24 |total_loss:  0.246494812756026 | epoch_duration:  0.75 sec\n",
      "Epoch no. 25 |total_loss:  0.24535210998103865 | epoch_duration:  0.74 sec\n",
      "Epoch no. 26 |total_loss:  0.2438010462895543 | epoch_duration:  0.78 sec\n",
      "Epoch no. 27 |total_loss:  0.24251837969370219 | epoch_duration:  0.79 sec\n",
      "Epoch no. 28 |total_loss:  0.2410085220101999 | epoch_duration:  0.74 sec\n",
      "Epoch no. 29 |total_loss:  0.23977763328673338 | epoch_duration:  0.75 sec\n",
      "Epoch no. 30 |total_loss:  0.23867265201284704 | epoch_duration:  0.78 sec\n",
      "Epoch no. 31 |total_loss:  0.23789689619361218 | epoch_duration:  0.74 sec\n",
      "Epoch no. 32 |total_loss:  0.23688403329944352 | epoch_duration:  0.78 sec\n",
      "Epoch no. 33 |total_loss:  0.2355185015843724 | epoch_duration:  0.8 sec\n",
      "Epoch no. 34 |total_loss:  0.23463839341243692 | epoch_duration:  0.77 sec\n",
      "Epoch no. 35 |total_loss:  0.2331230260338676 | epoch_duration:  0.78 sec\n",
      "Epoch no. 36 |total_loss:  0.2327316156960472 | epoch_duration:  0.75 sec\n",
      "Epoch no. 37 |total_loss:  0.23164582667727115 | epoch_duration:  0.76 sec\n",
      "Epoch no. 38 |total_loss:  0.23108489040710709 | epoch_duration:  0.8 sec\n",
      "Epoch no. 39 |total_loss:  0.2300298777804088 | epoch_duration:  0.78 sec\n",
      "Epoch no. 40 |total_loss:  0.22967438430990383 | epoch_duration:  0.76 sec\n",
      "Epoch no. 41 |total_loss:  0.22839295744665847 | epoch_duration:  0.78 sec\n",
      "Epoch no. 42 |total_loss:  0.22850193769423943 | epoch_duration:  0.78 sec\n",
      "Epoch no. 43 |total_loss:  0.22749602326906793 | epoch_duration:  0.76 sec\n",
      "Epoch no. 44 |total_loss:  0.22709357748923126 | epoch_duration:  0.78 sec\n",
      "Epoch no. 45 |total_loss:  0.22651498275198229 | epoch_duration:  0.78 sec\n",
      "Epoch no. 46 |total_loss:  0.226352524595922 | epoch_duration:  0.74 sec\n",
      "Epoch no. 47 |total_loss:  0.22575141037964608 | epoch_duration:  0.76 sec\n",
      "Epoch no. 48 |total_loss:  0.22518885054747606 | epoch_duration:  0.77 sec\n",
      "Epoch no. 49 |total_loss:  0.22460037975901911 | epoch_duration:  0.75 sec\n",
      "Epoch no. 50 |total_loss:  0.22445512663259523 | epoch_duration:  0.75 sec\n",
      "Epoch no. 51 |total_loss:  0.2239132614285494 | epoch_duration:  0.79 sec\n",
      "Epoch no. 52 |total_loss:  0.22382260762686063 | epoch_duration:  0.77 sec\n",
      "R2 tensor(0.9570)\n"
     ]
    }
   ],
   "source": [
    "features = [\"TMIN\",\"temp_forecast\",\"ry\",\"max_rh\",\"Week_av\",\"2Week_av\",\"Month_av\"]\n",
    "for i in range(1,7):\n",
    "    features.append(\"d\" + str(i))\n",
    "model = NN(29,24,12)\n",
    "model.load_data(\"for_1965_2024.csv\",features,\"TMAX\")\n",
    "model.train()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NYC,2024-03-18,48.0\n",
      "MIA,2024-03-18,74.0\n",
      "MDW,2024-03-18,\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[153], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvars.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     26\u001b[0m         json\u001b[38;5;241m.\u001b[39mdump(data,f)\n\u001b[1;32m---> 27\u001b[0m \u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m18\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m#data = \u001b[39;00m\n",
      "Cell \u001b[1;32mIn[153], line 22\u001b[0m, in \u001b[0;36mupdate\u001b[1;34m(day)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;28mprint\u001b[39m(line)\n\u001b[1;32m---> 22\u001b[0m         newlist\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     23\u001b[0m     data[city] \u001b[38;5;241m=\u001b[39m newlist\n\u001b[0;32m     24\u001b[0m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprev_day\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m day\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: ''"
     ]
    }
   ],
   "source": [
    "import json\n",
    "def update(day):\n",
    "    nyurl = \"https://mesonet.agron.iastate.edu/cgi-bin/request/daily.py?network=NY_ASOS&stations=NYC\"\n",
    "    miaurl = \"https://mesonet.agron.iastate.edu/cgi-bin/request/daily.py?network=FL_ASOS&stations=MIA\"\n",
    "    chiurl = \"https://mesonet.agron.iastate.edu/cgi-bin/request/daily.py?network=IL_ASOS&stations=MDW\"\n",
    "    ausurl = \"https://mesonet.agron.iastate.edu/cgi-bin/request/daily.py?network=TX_ASOS&stations=AUS\"\n",
    "    with open('vars.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "        pday = data[\"prev_day\"]\n",
    "        diff = day - pday\n",
    "        if diff <= 0:\n",
    "            return\n",
    "        url = \"&year1=2024&month1=3&day1=\" + str(pday + 1) + \"&year2=2024&month2=3&day2=\" + str(day) + \"&var=max_temp_f&na=blank&format=csv\"\n",
    "        for city in [\"NY\",\"M\",\"C\",\"A\"]:\n",
    "            u = {\"NY\" : nyurl,\"M\" : miaurl, \"C\" : chiurl,\"A\" : ausurl}[city] + url\n",
    "            r = requests.get(u).text\n",
    "            lines = r.split(\"\\n\")[1:-1]\n",
    "            newlist = data[city]\n",
    "            newlist = newlist[diff:]\n",
    "            for line in lines:\n",
    "                print(line)\n",
    "                newlist.append(float(line.split(\",\")[-1]))\n",
    "            data[city] = newlist\n",
    "        data[\"prev_day\"] = day\n",
    "    with open('vars.json', 'w') as f:\n",
    "        json.dump(data,f)\n",
    "day = 18\n",
    "update(day)\n",
    "with open('vars.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "M_daily,C_daily,A_daily,NY_daily = data[\"M\"],data[\"C\"],data[\"A\"],data[\"NY\"]\n",
    "data = pd.DataFrame({\"STATION\":['USW00012839', 'USW00014819', 'USW00013904', 'USW00094728'],\\\n",
    "              \"Month\":[3,3,3,3],\"days\" : [M_daily,C_daily,A_daily,NY_daily]})\n",
    "key = \"ade9eb48979c4310ae7b3ba195cfd999\"\n",
    "r = requests.get(url,\n",
    "                 params={'param': ['temperature'],\n",
    "                         'lat': [40.78,41.7,30.2,25.8],\n",
    "                         'lon': [-74,-87.7,-97.7,80.3],\n",
    "                         'location_id': ['NY','C','A','M'],\n",
    "                         'freq' : 'D',\n",
    "                         'start': '2024-03-0' + str(day),\n",
    "                         'end': '2024-03-0' + str(day)},\n",
    "                 headers={'api-key': key3}\n",
    "                 )\n",
    "a = ast.literal_eval(ast.literal_eval(r.text)[\"data\"])\n",
    "a = pd.DataFrame(columns = a[\"columns\"], data = a[\"data\"])\n",
    "\n",
    "for i,row in data.iterrows():\n",
    "    data.at[i,\"d1\"],data.at[i,\"d2\"],data.at[i,\"d3\"],data.at[i,\"d4\"],data.at[i,\"d5\"],data.at[i,\"d6\"],data.at[i,\"d7\"] = track[-7:]\n",
    "    data[[\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"]] = \\\n",
    "            data.apply(lambda x : self.month_fun(x[\"Month\"]),axis = 1,result_type=\"expand\")\n",
    "    data[['USW00012839', 'USW00014819', 'USW00013904', 'USW00094728']] =\\\n",
    "              data.apply(lambda x : self.stat_fun(x[\"STATION\"]),axis = 1, result_type=\"expand\")\n",
    "    data.drop([\"Month\",\"STATION\",\"days\"],axis = 1,inplace = True)\n",
    "    url = \"&year1=2024&month1=3&day1=\" + str(day) + \"&year2=2024&month2=3&day2=\" + str(day) + \"&var=min_temp_f&var=precip_in&var=max_rh&na=blank&format=csv\"\n",
    "    ud = { 'USW00012839' : \"https://mesonet.agron.iastate.edu/cgi-bin/request/daily.py?network=NY_ASOS&stations=NYC\",\\\n",
    "     'USW00014819' : \"https://mesonet.agron.iastate.edu/cgi-bin/request/daily.py?network=FL_ASOS&stations=MIA\",\\\n",
    "      'USW00013904' : \"https://mesonet.agron.iastate.edu/cgi-bin/request/daily.py?network=IL_ASOS&stations=MDW\",\\\n",
    "     'USW00094728' : \"https://mesonet.agron.iastate.edu/cgi-bin/request/daily.py?network=TX_ASOS&stations=AUS\"}\n",
    "    url = ud[row[\"STATION\"]] + url\n",
    "    r = requests.get(url).text\n",
    "    data.at[i,\"TMIN\"] = float(r.split(\"\\n\")[1].split(\",\")[-3])\n",
    "    data.at[i,\"max_rh\"] = float(r.split(\"\\n\")[1].split(\",\")[-1])\n",
    "    data.at[i,\"ry\"] = float(r.split(\"\\n\")[1].split(\",\")[-2])\n",
    "    tick = {3:'NY',1:'C',2:'A',0:'M'}[i]\n",
    "    data.at[i,\"temp_forecast\"] =(a[a[\"location_id (id)\"] == tick].loc[0][\"temperature (degC)\"] * 1.8) + 32\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch no. 1 |total_loss:  94325.65456734126 | epoch_duration:  1.79 sec\n",
      "Epoch no. 2 |total_loss:  33262.54662403601 | epoch_duration:  1.81 sec\n",
      "Epoch no. 3 |total_loss:  31194.10636482297 | epoch_duration:  1.74 sec\n",
      "Epoch no. 4 |total_loss:  29860.688218314775 | epoch_duration:  1.81 sec\n",
      "Epoch no. 5 |total_loss:  28860.173423635784 | epoch_duration:  1.72 sec\n",
      "Epoch no. 6 |total_loss:  27886.6012325522 | epoch_duration:  1.78 sec\n",
      "Epoch no. 7 |total_loss:  27251.314880143695 | epoch_duration:  1.83 sec\n",
      "Epoch no. 8 |total_loss:  26081.98154727582 | epoch_duration:  1.82 sec\n",
      "Epoch no. 9 |total_loss:  25185.2185729402 | epoch_duration:  1.85 sec\n",
      "Epoch no. 10 |total_loss:  24476.975446796307 | epoch_duration:  1.89 sec\n",
      "Epoch no. 11 |total_loss:  24198.89677720521 | epoch_duration:  1.85 sec\n",
      "Epoch no. 12 |total_loss:  23747.9936821425 | epoch_duration:  1.88 sec\n",
      "Epoch no. 13 |total_loss:  23339.136313121744 | epoch_duration:  1.85 sec\n",
      "Epoch no. 14 |total_loss:  23149.65516880386 | epoch_duration:  1.79 sec\n",
      "Epoch no. 15 |total_loss:  23022.373804675943 | epoch_duration:  1.82 sec\n",
      "Epoch no. 16 |total_loss:  23006.84989200411 | epoch_duration:  1.87 sec\n",
      "Epoch no. 17 |total_loss:  22719.279689182793 | epoch_duration:  1.89 sec\n",
      "Epoch no. 18 |total_loss:  22673.518452820066 | epoch_duration:  1.97 sec\n",
      "Epoch no. 19 |total_loss:  22591.1070369963 | epoch_duration:  1.91 sec\n",
      "Epoch no. 20 |total_loss:  22429.966499623806 | epoch_duration:  1.94 sec\n",
      "Epoch no. 21 |total_loss:  22362.99051980182 | epoch_duration:  1.93 sec\n",
      "Epoch no. 22 |total_loss:  22233.918282239818 | epoch_duration:  1.92 sec\n",
      "Epoch no. 23 |total_loss:  22224.008228577626 | epoch_duration:  1.96 sec\n",
      "R2 tensor(0.9224)\n"
     ]
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'balance': 9389}\n",
      "{'exchange_active': True, 'trading_active': True}\n",
      "HIGHNY-24MAR16-B61.5\n"
     ]
    }
   ],
   "source": [
    "#client = ExchangeClient(\"https://demo.kalshi.co/\",\"mlevine6@bu.edu\",\"in5u>e@t}MaE:mM\")\n",
    "config = kalshi_python.Configuration()\n",
    "# Comment the line below to use production\n",
    "config.host = 'https://demo-api.kalshi.co/trade-api/v2'\n",
    "\n",
    "# Create an API configuration passing your credentials.\n",
    "# Use this if you want the kalshi_python sdk to manage the authentication for you.\n",
    "kalshi_api = kalshi_python.ApiInstance(\n",
    "    email=\"mlevine6@bu.edu\",\n",
    "    password= \"in5u>e@t}MaE:mM\",\n",
    "    configuration=config,\n",
    ")\n",
    "balanceResponse = kalshi_api.get_balance()\n",
    "exchangeStatus = kalshi_api.get_exchange_status()\n",
    "print(balanceResponse)\n",
    "print(exchangeStatus)\n",
    "tickers = [\"HIGHNY-24MAR16\",\"highchi\",\"highmia\",\"highaus\"]\n",
    "orderbookResponse = kalshi_api.get_event(tickers[0])\n",
    "# print(orderbookResponse)\n",
    "markets = orderbookResponse.markets\n",
    "for o in orderbookResponse.markets:\n",
    "    if o.result == \"yes\":\n",
    "        print(o.ticker)\n",
    "temps = [float(o.ticker.split(\"-\")[-1][1:]) for o in markets]\n",
    "# diffs = [abs(pred-t) for t in temps]\n",
    "# if exchangeStatus.trading_active:\n",
    "#     # Submit an order for 10 yes contracts at 50cents on 'FED-23DEC-T3.00'.\n",
    "#     orderUuid = str(uuid.uuid4())\n",
    "#     orderResponse = kalshi_api.create_order(CreateOrderRequest(\n",
    "#         ticker=tickers[0],\n",
    "#         action='buy',\n",
    "#         type='limit',\n",
    "#         yes_price=50,\n",
    "#         count=1,\n",
    "#         client_order_id=orderUuid,\n",
    "#         side='yes',\n",
    "#     ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.0\n",
      "\n",
      "3\n",
      "['station,day,max_temp_f', 'NYC,2024-03-17,63.0', '']\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "nyurl = \"https://mesonet.agron.iastate.edu/cgi-bin/request/daily.py?network=NY_ASOS&stations=NYC\"\n",
    "miaurl = \"https://mesonet.agron.iastate.edu/cgi-bin/request/daily.py?network=FL_ASOS&stations=MIA\"\n",
    "chiurl = \"https://mesonet.agron.iastate.edu/cgi-bin/request/daily.py?network=IL_ASOS&stations=MDW\"\n",
    "ausurl = \"https://mesonet.agron.iastate.edu/cgi-bin/request/daily.py?network=TX_ASOS&stations=AUS\"\n",
    "day = 17\n",
    "\n",
    "# r = requests.get(\"https://forecast.weather.gov/product.php?site=NWS&issuedby=NYC&product=CLI&format=CI&version=1&glossary=1&highlight=off\").text\n",
    "# table = r.split(\"...................................................................\")[1]\n",
    "# maximum = int(table.split(\"MAXIMUM\")[1].split()[0])\n",
    "# minimum = table.split(\"MINIMUM\")[1].split()[0])\n",
    "\n",
    "# parser= BeautifulSoup(r,\"html.parser\")\n",
    "r = requests.get(\"https://mesonet.agron.iastate.edu/cgi-bin/request/daily.py?network=NY_ASOS&stations=NYC&year1=2024&month1=3&day1=\" + str(day) + \"&year2=2024&month2=3&day2=\" + str(day) + \"&var=max_temp_f&na=blank&format=csv\").text\n",
    "print(r.split(\",\")[-1])\n",
    "print(len(r.split(\"\\n\")))\n",
    "print(r.split(\"\\n\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def update(day):\n",
    "    nyurl = \"https://mesonet.agron.iastate.edu/cgi-bin/request/daily.py?network=NY_ASOS&stations=NYC\"\n",
    "    miaurl = \"https://mesonet.agron.iastate.edu/cgi-bin/request/daily.py?network=FL_ASOS&stations=MIA\"\n",
    "    chiurl = \"https://mesonet.agron.iastate.edu/cgi-bin/request/daily.py?network=IL_ASOS&stations=MDW\"\n",
    "    ausurl = \"https://mesonet.agron.iastate.edu/cgi-bin/request/daily.py?network=TX_ASOS&stations=AUS\"\n",
    "    with open('vars.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "        pday = data[\"prev_day\"]\n",
    "        diff = day - pday\n",
    "        if diff <= 0:\n",
    "            return\n",
    "        url = \"&year1=2024&month1=3&day1=\" + str(pday + 1) + \"&year2=2024&month2=3&day2=\" + str(day) + \"&var=max_temp_f&na=blank&format=csv\"\n",
    "        for city in [\"NY\",\"M\",\"C\",\"A\"]:\n",
    "            u = {\"NY\" : nyurl,\"M\" : miaurl, \"C\" : chiurl,\"A\" : ausurl}[city] + url\n",
    "            r = requests.get(u).text\n",
    "            lines = r.split(\"\\n\")[1:-1]\n",
    "            newlist = data[city]\n",
    "            newlist = newlist[diff:]\n",
    "            for line in lines:\n",
    "                newlist.append(float(line.split(\",\")[-1]))\n",
    "            data[city] = newlist\n",
    "        data[\"prev_day\"] = day\n",
    "    with open('vars.json', 'w') as f:\n",
    "        json.dump(data,f)\n",
    "            \n",
    "# NY_daily = [43,36,40,41,40,43,45,47,44,41,55,55,62,43,48,55,68,59,49,53,54,57,49,51,52,66,62,74,73,61]\n",
    "# M_daily =  [79,84,76,70,70,74,77,79,76,77,76,78,80,81,79,80,83,85,83,82,86,85,86,88,80,80,81,84,84,86]\n",
    "# C_daily =  [33,30,47,52,58,68,61,52,36,62,72,74,53,46,47,60,74,72,45,50,50,47,47,49,68,72,69,67,52,60]\n",
    "# A_daily =  [73,52,54,68,79,77,87,75,79,83,82,80,70,49,68,81,80,83,91,83,75,83,62,67,73,77,84,80,76,75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# df = pd.read_csv(\"1965_2024.csv\")\n",
    "# for at in [\"TMIN\",\"ry\",\"max_rh\",\"Week_av\",\"2Week_av\",\"Month_av\",\"d1\",\"d2\",\"d3\",\"d4\",\"d5\",\"d6\",\"d7\"]:\n",
    "#     plt.figure(figsize=(9,6))\n",
    "#     plt.subplot(1,2,1)\n",
    "#     plt.scatter(df[at],df[\"TMAX\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "key = \"ade9eb48979c4310ae7b3ba195cfd999\"\n",
    "key2 = \"2b13ca426b96408d9d0c1f3814d15390\"\n",
    "key0 = \"a5c8c0da0c5f449fb21e280d3b6dbd3a\"\n",
    "key3 = \"bde9418ce268472293a3d35791409fbc\"\n",
    "url = 'https://api.oikolab.com/weather'\n",
    "\n",
    "r = requests.get(url,\n",
    "                 params={'param': ['temperature'],\n",
    "                         'lat': [40.78,41.7,30.2,25.8],\n",
    "                         'lon': [-74,-87.7,-97.7,80.3],\n",
    "                         'location_id': ['NY','C','A','M'],\n",
    "                         'freq' : 'D',\n",
    "                         'start': '2000-01-01',\n",
    "                         'end': '2023-12-31'},\n",
    "                 headers={'api-key': key3}\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      coordinates (lat,lon) location_id (id) model (name)  \\\n",
      "0              (25.8, 80.3)                M         era5   \n",
      "1              (25.8, 80.3)                M         era5   \n",
      "2              (25.8, 80.3)                M         era5   \n",
      "3              (25.8, 80.3)                M         era5   \n",
      "4              (25.8, 80.3)                M         era5   \n",
      "...                     ...              ...          ...   \n",
      "35059         (41.7, -87.7)                C         era5   \n",
      "35060         (41.7, -87.7)                C         era5   \n",
      "35061         (41.7, -87.7)                C         era5   \n",
      "35062         (41.7, -87.7)                C         era5   \n",
      "35063         (41.7, -87.7)                C         era5   \n",
      "\n",
      "       model elevation (surface)  utc_offset (hrs)  temperature (degC)  \\\n",
      "0                         117.83               5.5               14.03   \n",
      "1                         117.83               5.5               12.91   \n",
      "2                         117.83               5.5               12.20   \n",
      "3                         117.83               5.5               12.92   \n",
      "4                         117.83               5.5               12.18   \n",
      "...                          ...               ...                 ...   \n",
      "35059                     196.09              -6.0                2.65   \n",
      "35060                     196.09              -6.0                3.65   \n",
      "35061                     196.09              -6.0                4.19   \n",
      "35062                     196.09              -6.0                1.05   \n",
      "35063                     196.09              -6.0                1.08   \n",
      "\n",
      "        datetime  \n",
      "0     2000-01-01  \n",
      "1     2000-01-02  \n",
      "2     2000-01-03  \n",
      "3     2000-01-04  \n",
      "4     2000-01-05  \n",
      "...          ...  \n",
      "35059 2023-12-27  \n",
      "35060 2023-12-28  \n",
      "35061 2023-12-29  \n",
      "35062 2023-12-30  \n",
      "35063 2023-12-31  \n",
      "\n",
      "[35064 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "import ast\n",
    "a = ast.literal_eval(ast.literal_eval(r.text)[\"data\"])\n",
    "# print(ast.literal_eval(a[\"data\"])[\"data\"])\n",
    "a = pd.DataFrame(columns = a[\"columns\"], data = a[\"data\"])\n",
    "# ny = a[a[\"location_id (id)\"] == \"NY\"]\n",
    "# au = a[a[\"location_id (id)\"] == \"A\"]\n",
    "# m  = a[a[\"location_id (id)\"] == \"M\"]\n",
    "# c  = a[a[\"location_id (id)\"] == \"C\"]\n",
    "# dfs = []\n",
    "# for df in [ny,au,m,c]:\n",
    "for i, row in a.iterrows():\n",
    "    a.at[i,\"datetime\"] = dt.datetime(2000,1,1) + dt.timedelta(days = i%(len(a)/4))\n",
    "#     dfs.append(df)\n",
    "# dfs = pd.concat(dfs)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.to_csv(\"forecasts2000_2023.csv\",index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
